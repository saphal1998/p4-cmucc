{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "122d5cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types.{StructType, StructField, StringType}\n",
       "import org.apache.spark.sql.functions.col\n",
       "import org.apache.spark.sql._\n",
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.{StructType, StructField, StringType}\n",
    "import org.apache.spark.sql.functions.{col}\n",
    "import org.apache.spark.sql._\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65eeb214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graph: Seq[(String, String)] = List((1,2), (1,3), (2,3), (2,1))\n",
       "graphRDD: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[4] at parallelize at <console>:42\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val graph = Seq((\"1\",\"2\"),(\"1\",\"3\"),(\"2\",\"3\"),(\"2\",\"1\"))\n",
    "val graphRDD = sc.parallelize(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f63ff029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seqOp: (List[String], String) => List[String]\n",
       "combOp: (List[String], List[String]) => List[String]\n",
       "zeroVal: Unit = ()\n",
       "following: org.apache.spark.rdd.RDD[(String, List[String])] = ShuffledRDD[5] at aggregateByKey at <console>:49\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def seqOp = (accumulator: List[String], element: String) => \n",
    "    accumulator:+element\n",
    "\n",
    "def combOp = (accumulator1: List[String], accumulator2: List[String]) => \n",
    "    accumulator1 ++ accumulator2\n",
    "\n",
    "val zeroVal = ()\n",
    "val following = graphRDD.aggregateByKey(List[String]())(seqOp, combOp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "015a269c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Array[(String, List[String])] = Array((1,List(2, 3)), (2,List(3, 1)))\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "following.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e9618a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seqOp: (List[String], String) => List[String]\n",
       "combOp: (List[String], List[String]) => List[String]\n",
       "zeroVal: Unit = ()\n",
       "followers: org.apache.spark.rdd.RDD[(String, List[String])] = ShuffledRDD[7] at aggregateByKey at <console>:49\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def seqOp = (accumulator: List[String], element: String) => \n",
    "    accumulator:+element\n",
    "\n",
    "def combOp = (accumulator1: List[String], accumulator2: List[String]) => \n",
    "    accumulator1 ++ accumulator2\n",
    "\n",
    "val zeroVal = ()\n",
    "val followers = graphRDD.map{case(k,v) => (v,k)}.aggregateByKey(List[String]())(seqOp, combOp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cf13540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Array[(String, List[String])] = Array((1,List(2)), (2,List(1)), (3,List(1, 2)))\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "followers.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ab86509",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "51: error: value size is not a member of org.apache.spark.rdd.RDD[(String, List[String])]",
     "output_type": "error",
     "traceback": [
      "<console>:51: error: value size is not a member of org.apache.spark.rdd.RDD[(String, List[String])]",
      "           val size = following.size",
      "                                ^",
      "<console>:55: error: value values is not a member of org.apache.spark.rdd.RDD[Nothing]",
      "         val danglingContribs = (1 - contribs.values.reduce(_ + _)) / numVertices",
      "                                              ^",
      "<console>:56: error: value reduceByKey is not a member of org.apache.spark.rdd.RDD[Nothing]",
      "         contribs = contribs.reduceByKey(_ + _).mapValues(x => 0.15 / numVertices + 0.85 * (x + danglingContribs))",
      "                             ^",
      "<console>:60: error: type mismatch;",
      " found   : org.apache.spark.rdd.RDD[Nothing]",
      " required: org.apache.spark.rdd.RDD[(String, Double)]",
      "Note: Nothing <: (String, Double), but class RDD is invariant in type T.",
      "You may wish to define T as +T instead. (SLS 4.5)",
      "         ranks = contribs.union(sourceContribs)",
      "                               ^",
      ""
     ]
    }
   ],
   "source": [
    "val users = graphRDD.flatMap{ case (k,v) => List(k,v) }.distinct\n",
    "val sourceUsers = users.subtract(following.keys)\n",
    "val numVertices = 4\n",
    "\n",
    "var ranks = followers.mapValues(v => 1.0 / numVertices)\n",
    "var iterations = 10\n",
    "\n",
    "for (i <- 1 to iterations) {\n",
    "  var contribs = followers.join(ranks).values.flatMap { case (followees, rank) =>\n",
    "    val size = following.size\n",
    "    followees.map(followee => (followee, rank / size))\n",
    "  }\n",
    "\n",
    "  val danglingContribs = (1 - contribs.values.reduce(_ + _)) / numVertices\n",
    "  contribs = contribs.reduceByKey(_ + _).mapValues(x => 0.15 / numVertices + 0.85 * (x + danglingContribs))\n",
    "\n",
    "  val sourceContribs = sourceUsers.map(x => (x, 0.15 / numVertices + 0.85 * (0 + danglingContribs)))\n",
    "\n",
    "  ranks = contribs.union(sourceContribs)\n",
    "}\n",
    "\n",
    "val output = ranks.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fdd59d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
