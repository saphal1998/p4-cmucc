{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "122d5cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://10.0.0.232:4041\n",
       "SparkContext available as 'sc' (version = 3.3.2, master = local[*], app id = local-1679770721642)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types.{StructType, StructField, StringType}\n",
       "import org.apache.spark.sql.functions.col\n",
       "import org.apache.spark.sql._\n",
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.{StructType, StructField, StringType}\n",
    "import org.apache.spark.sql.functions.{col}\n",
    "import org.apache.spark.sql._\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65eeb214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graph: Seq[(String, String)] = List((1,2), (1,3), (2,3), (2,1))\n",
       "graphRDD: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[1] at parallelize at <console>:34\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val graph = Seq((\"1\",\"2\"),(\"1\",\"3\"),(\"2\",\"3\"),(\"2\",\"1\"))\n",
    "val graphRDD = sc.parallelize(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f63ff029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seqOp: (List[String], String) => List[String]\n",
       "combOp: (List[String], List[String]) => List[String]\n",
       "zeroVal: Unit = ()\n",
       "following: org.apache.spark.rdd.RDD[(String, List[String])] = ShuffledRDD[16] at aggregateByKey at <console>:41\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def seqOp = (accumulator: List[String], element: String) => \n",
    "    accumulator:+element\n",
    "\n",
    "def combOp = (accumulator1: List[String], accumulator2: List[String]) => \n",
    "    accumulator1 ++ accumulator2\n",
    "\n",
    "val zeroVal = ()\n",
    "val following = graphRDD.aggregateByKey(List[String]())(seqOp, combOp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "015a269c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: Array[(String, List[String])] = Array((1,List(2, 3)), (2,List(3, 1)))\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "following.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e9618a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seqOp: (List[String], String) => List[String]\n",
       "combOp: (List[String], List[String]) => List[String]\n",
       "zeroVal: Unit = ()\n",
       "followers: org.apache.spark.rdd.RDD[(String, List[String])] = ShuffledRDD[18] at aggregateByKey at <console>:41\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def seqOp = (accumulator: List[String], element: String) => \n",
    "    accumulator:+element\n",
    "\n",
    "def combOp = (accumulator1: List[String], accumulator2: List[String]) => \n",
    "    accumulator1 ++ accumulator2\n",
    "\n",
    "val zeroVal = ()\n",
    "val followers = graphRDD.map{case(k,v) => (v,k)}.aggregateByKey(List[String]())(seqOp, combOp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4cf13540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: Array[(String, List[String])] = Array((1,List(2)), (2,List(1)), (3,List(1, 2)))\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "followers.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "232fb9d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "users: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[62] at distinct at <console>:34\n",
       "user_count: org.apache.spark.broadcast.Broadcast[Long] = Broadcast(47)\n",
       "res28: Array[String] = Array(1, 2, 3)\n"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val users = graphRDD.flatMap{ case (k,v) => List(k,v) }.distinct\n",
    "val user_count = sc.broadcast(users.count())\n",
    "users.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "911d495c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res29: Long = 3\n"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_count.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6ce7cbcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ranks: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[80] at rdd at <console>:33\n"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ranks = users.toDF().withColumn(\"initial_rank\", lit(1.0/user_count.value)).rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "950ffe7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res34: Array[org.apache.spark.sql.Row] = Array([1,0.3333333333333333], [2,0.3333333333333333], [3,0.3333333333333333])\n"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
