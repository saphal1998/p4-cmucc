{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c54bd6d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://10.0.0.232:4041\n",
       "SparkContext available as 'sc' (version = 3.3.2, master = local[*], app id = local-1679770721642)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types.{StructType, StructField, StringType}\n",
       "import org.apache.spark.sql.functions.col\n",
       "import org.apache.spark.sql._\n",
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.{StructType, StructField, StringType}\n",
    "import org.apache.spark.sql.functions.{col}\n",
    "import org.apache.spark.sql._\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b75abc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graph: Seq[(String, String)] = List((1,2), (1,3), (2,3), (2,1))\n",
       "graphRDD: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[1] at parallelize at <console>:34\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val graph = Seq((\"1\",\"2\"),(\"1\",\"3\"),(\"2\",\"3\"),(\"2\",\"1\"))\n",
    "val graphRDD = sc.parallelize(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2eb6ef3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seqOp: (List[String], String) => List[String]\n",
       "combOp: (List[String], List[String]) => List[String]\n",
       "zeroVal: Unit = ()\n",
       "following: org.apache.spark.rdd.RDD[(String, List[String])] = ShuffledRDD[16] at aggregateByKey at <console>:41\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def seqOp = (accumulator: List[String], element: String) => \n",
    "    accumulator:+element\n",
    "\n",
    "def combOp = (accumulator1: List[String], accumulator2: List[String]) => \n",
    "    accumulator1 ++ accumulator2\n",
    "\n",
    "val zeroVal = ()\n",
    "val following = graphRDD.aggregateByKey(List[String]())(seqOp, combOp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e798b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: Array[(String, List[String])] = Array((1,List(2, 3)), (2,List(3, 1)))\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "following.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79ad6b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seqOp: (List[String], String) => List[String]\n",
       "combOp: (List[String], List[String]) => List[String]\n",
       "zeroVal: Unit = ()\n",
       "followers: org.apache.spark.rdd.RDD[(String, List[String])] = ShuffledRDD[18] at aggregateByKey at <console>:41\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def seqOp = (accumulator: List[String], element: String) => \n",
    "    accumulator:+element\n",
    "\n",
    "def combOp = (accumulator1: List[String], accumulator2: List[String]) => \n",
    "    accumulator1 ++ accumulator2\n",
    "\n",
    "val zeroVal = ()\n",
    "val followers = graphRDD.map{case(k,v) => (v,k)}.aggregateByKey(List[String]())(seqOp, combOp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da6c754a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: Array[(String, List[String])] = Array((1,List(2)), (2,List(1)), (3,List(1, 2)))\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "followers.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4f15e4a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "users: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[46] at distinct at <console>:34\n",
       "user_count: org.apache.spark.broadcast.Broadcast[Long] = Broadcast(36)\n",
       "res20: Array[String] = Array(1, 2, 3)\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val users = graphRDD.flatMap{ case (k,v) => List(k,v) }.distinct\n",
    "val user_count = sc.broadcast(users.count())\n",
    "users.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5b06e6de",
   "metadata": {},
   "outputs": [
    {
     "ename": "java.lang.InternalError",
     "evalue": " java.lang.IllegalAccessException: final field has no write access: $Lambda$3774/0x0000000801d93218.arg$1/putField, from class java.lang.Object (module java.base)",
     "output_type": "error",
     "traceback": [
      "java.lang.InternalError: java.lang.IllegalAccessException: final field has no write access: $Lambda$3774/0x0000000801d93218.arg$1/putField, from class java.lang.Object (module java.base)",
      "  at java.base/jdk.internal.reflect.MethodHandleAccessorFactory.newFieldAccessor(MethodHandleAccessorFactory.java:167)",
      "  at java.base/jdk.internal.reflect.ReflectionFactory.newFieldAccessor(ReflectionFactory.java:145)",
      "  at java.base/java.lang.reflect.Field.acquireOverrideFieldAccessor(Field.java:1184)",
      "  at java.base/java.lang.reflect.Field.getOverrideFieldAccessor(Field.java:1153)",
      "  at java.base/java.lang.reflect.Field.set(Field.java:820)",
      "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:406)",
      "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:163)",
      "  at org.apache.spark.SparkContext.clean(SparkContext.scala:2501)",
      "  at org.apache.spark.rdd.RDD.$anonfun$map$1(RDD.scala:414)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)",
      "  at org.apache.spark.rdd.RDD.map(RDD.scala:413)",
      "  ... 38 elided",
      "Caused by: java.lang.IllegalAccessException: final field has no write access: $Lambda$3774/0x0000000801d93218.arg$1/putField, from class java.lang.Object (module java.base)",
      "  at java.base/java.lang.invoke.MemberName.makeAccessException(MemberName.java:955)",
      "  at java.base/java.lang.invoke.MethodHandles$Lookup.unreflectField(MethodHandles.java:3511)",
      "  at java.base/java.lang.invoke.MethodHandles$Lookup.unreflectSetter(MethodHandles.java:3502)",
      "  at java.base/java.lang.invoke.MethodHandleImpl$1.unreflectField(MethodHandleImpl.java:1630)",
      "  at java.base/jdk.internal.reflect.MethodHandleAccessorFactory.newFieldAccessor(MethodHandleAccessorFactory.java:145)",
      "  ... 50 more",
      ""
     ]
    }
   ],
   "source": [
    "def generate_ranks = (row: String) => {\n",
    "    (row, 1.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "80da4586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res18: Array[String] = Array(1, 2, 3)\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
