{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce06e679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://10.0.0.232:4040\n",
       "SparkContext available as 'sc' (version = 3.3.2, master = local[*], app id = local-1679803924657)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types.{StructType, StructField, StringType}\n",
       "import org.apache.spark.sql.functions.{col, lit}\n",
       "import org.apache.spark.sql._\n",
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.{StructType, StructField, StringType}\n",
    "import org.apache.spark.sql.functions.{col, lit}\n",
    "import org.apache.spark.sql._\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47b59c27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "load_data: ()org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\n",
       "graphDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [follower: string, followee: string]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data() = {\n",
    "    val graph = Seq(Row(\"0\",\"2\"),Row(\"1\",\"0\"),Row(\"1\",\"2\"),Row(\"1\",\"3\"), Row(\"2\", \"3\"))\n",
    "    val graphDF = spark.createDataFrame(\n",
    "        sc.parallelize(graph), \n",
    "        StructType(\n",
    "            List(\n",
    "                StructField(\"follower\", StringType), \n",
    "                StructField(\"followee\", StringType)\n",
    "            )\n",
    "        )\n",
    "    ).as(\"social_graph\")\n",
    "    graphDF.cache()\n",
    "    \n",
    "    graphDF\n",
    "}\n",
    "\n",
    "val graphDF = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad17a50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_users: (graphDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\n",
       "users: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [user_id: string]\n",
       "user_count: org.apache.spark.broadcast.Broadcast[Long] = Broadcast(3)\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_users(graphDF: DataFrame) = {\n",
    "    graphDF.select(col(\"followee\"))\n",
    "        .union(graphDF.select(col(\"follower\")))\n",
    "        .withColumnRenamed(\"followee\",\"user_id\").distinct.as(\"users\")\n",
    "}\n",
    "\n",
    "\n",
    "val users = get_users(graphDF)\n",
    "val user_count = sc.broadcast(users.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f3c3dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initialize_ranks: (users: org.apache.spark.sql.DataFrame)org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\n",
       "rank: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [user_id: string, rank_value: double]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initialize_ranks(users: DataFrame) = {\n",
    "    users.select(col(\"user_id\"), lit(1.0/user_count.value).as(\"rank_value\")).as(\"rank\")\n",
    "}\n",
    "\n",
    "val rank = initialize_ranks(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62438262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_followers_per_user: (social_graph: org.apache.spark.sql.DataFrame)org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\n",
       "followers: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [followee: string, followers: array<string>]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_followers_per_user(social_graph: DataFrame) = {\n",
    "    social_graph.groupBy(\"followee\").agg(collect_list(\"follower\").as(\"followers\")).as(\"followers\")\n",
    "}\n",
    "\n",
    "val followers = get_followers_per_user(graphDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5057a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_following_per_user: (social_graph: org.apache.spark.sql.DataFrame)org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\n",
       "following: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [follower: string, following: array<string>]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_following_per_user(social_graph: DataFrame) = {\n",
    "    social_graph.groupBy(\"follower\").agg(collect_list(\"followee\").as(\"following\")).as(\"following\")\n",
    "}\n",
    "val following = get_following_per_user(graphDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "350f9de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "enhanced_rank: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [user_id: string, rank_value: double ... 2 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var enhanced_rank = rank\n",
    "    .join(followers, col(\"followers.followee\") === col(\"rank.user_id\"), \"left\")\n",
    "    .select(col(\"user_id\").as(\"user_id\"), col(\"rank_value\"), col(\"followers\")).as(\"rank_followers\")\n",
    "    .join(following, col(\"following.follower\") === col(\"rank_followers.user_id\"), \"left\")\n",
    "    .select(col(\"user_id\"), col(\"rank_value\"), col(\"followers\"), col(\"following\")).as(\"rank_followers_following\")\n",
    "    .as(\"rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "270dc301",
   "metadata": {},
   "outputs": [],
   "source": [
    "// val non_dangling_users = following_followers_with_rank.filter(col(\"following\").isNotNull)\n",
    "// val dangling_users = following_followers_with_rank.filter(col(\"following\").isNull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dfe4f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_contributions: (rank: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n",
       "contributions: org.apache.spark.sql.DataFrame = [user_id: string, rank_value: double ... 3 more fields]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_contributions(rank: DataFrame) = {\n",
    "    rank.withColumn(\"contributions\", col(\"rank_value\") / when(col(\"following\").isNotNull, size(col(\"following\"))).otherwise(1))\n",
    "}\n",
    "\n",
    "val contributions = get_contributions(enhanced_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24331b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "explode_and_sum_contributions: (contributions: org.apache.spark.sql.DataFrame)org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\n",
       "summed_contributions: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [contribute_to: string, contributions: double]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def explode_and_sum_contributions(contributions: DataFrame) = {\n",
    "    val exploded_contribution = contributions.select(col(\"user_id\"),col(\"rank_value\"), col(\"followers\"), explode(col(\"following\")).as(\"contribute_to\"), col(\"contributions\")).as(\"exploded_contributions\")\n",
    "    exploded_contribution.groupBy(\"contribute_to\").agg(sum(\"contributions\").alias(\"contributions\")).as(\"summed_contributions\")\n",
    "}\n",
    "\n",
    "val summed_contributions = explode_and_sum_contributions(contributions)\n",
    "// val total_dangling_bonus = summed_contributions.select(\"contributions\").where(col(\"contribute_to\").isNull).first.getDouble(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a37c5d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calculate_rank: (contributions: org.apache.spark.sql.DataFrame, summed_contributions: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n",
       "new_ranks: org.apache.spark.sql.DataFrame = [user_id: string, followers: array<string> ... 2 more fields]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// def calculate_rank(contributions: DataFrame) = {\n",
    "//     val new_ranks_without_dangling = contributions\n",
    "//         .drop(\"contributions\").join(summed_contributions, col(\"contribute_to\") === col(\"user_id\"), \"left\")\n",
    "//         .drop(\"contribute_to\")        \n",
    "//         .select(col(\"user_id\"), col(\"followers\"), col(\"rank_value\"), col(\"following\"), col(\"contributions\").as(\"rank_contributions\"))\n",
    "//     val coalesce_ranking = new_ranks_without_dangling\n",
    "//             .withColumn(\"new_ranks\", when(col(\"following\").isNotNull, coalesce(col(\"rank_contributions\"), lit(0))).otherwise(col(\"rank_value\")))\n",
    "//     val dangling_user_contribution = coalesce_ranking.filter(col(\"following\").isNull).select(\"rank_contributions\").agg(sum(\"rank_contributions\")).first.getDouble(0)\n",
    "//     coalesce_ranking.withColumn(\"computed_rank\", col(\"new_ranks\") + dangling_user_contribution / user_count.value)\n",
    "//             .drop(\"rank_contributions\", \"new_ranks\")\n",
    "//             .withColumn(\"rank_value\", (col(\"computed_rank\") * 0.85) + (0.15 / user_count.value))\n",
    "//     coalesce_ranking\n",
    "// }\n",
    "\n",
    "def calculate_rank(contributions: DataFrame, summed_contributions: DataFrame) = {\n",
    "    val remainder = 1.0 - summed_contributions.agg(sum(\"contributions\"))\n",
    "    val new_ranks = contributions\n",
    "        .drop(\"contributions\")\n",
    "        .join(summed_contributions, col(\"contribute_to\") === col(\"user_id\"), \"left\").drop(\"contribute_to\")\n",
    "        .select(col(\"user_id\"), col(\"followers\"), col(\"rank_value\"), col(\"following\"), col(\"contributions\").as(\"rank_contributions\"))\n",
    "        .withColumn(\"final_rank\", (when(col(\"rank_contributions\").isNotNull, col(\"rank_contributions\")).otherwise(lit(0))) + remainder / user_count.value)\n",
    "        .withColumn(\"rank_value\", (col(\"final_rank\") * 0.85) + (0.15 / user_count.value))\n",
    "        .select(col(\"user_id\"), col(\"followers\"), col(\"following\"), col(\"rank_value\"))\n",
    "    new_ranks\n",
    "}\n",
    "val new_ranks = calculate_rank(contributions, summed_contributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6af6dab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/26 00:12:56 WARN DAGScheduler: Broadcasting large task binary with size 1018.0 KiB\n",
      "23/03/26 00:12:57 WARN DAGScheduler: Broadcasting large task binary with size 1004.3 KiB\n",
      "23/03/26 00:13:25 ERROR Utils: uncaught error in thread spark-listener-group-appStatus, stopping SparkContext\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3537)\n",
      "\tat java.base/java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:241)\n",
      "\tat java.base/java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:586)\n",
      "\tat java.base/java.lang.StringBuilder.append(StringBuilder.java:179)\n",
      "\tat scala.collection.mutable.StringBuilder.append(StringBuilder.scala:213)\n",
      "\tat scala.collection.TraversableOnce$appender$1.apply(TraversableOnce.scala:418)\n",
      "\tat scala.collection.TraversableOnce$appender$1.apply(TraversableOnce.scala:410)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.addString(TraversableOnce.scala:424)\n",
      "\tat scala.collection.TraversableOnce.addString$(TraversableOnce.scala:407)\n",
      "\tat scala.collection.AbstractIterator.addString(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.mkString(TraversableOnce.scala:377)\n",
      "\tat scala.collection.TraversableOnce.mkString$(TraversableOnce.scala:376)\n",
      "\tat scala.collection.AbstractIterator.mkString(Iterator.scala:1431)\n",
      "\tat scala.runtime.ScalaRunTime$._toString(ScalaRunTime.scala:165)\n",
      "\tat org.apache.spark.sql.execution.ui.SparkListenerSQLAdaptiveExecutionUpdate.toString(SQLListener.scala:31)\n",
      "\tat java.base/java.lang.String.valueOf(String.java:4225)\n",
      "\tat java.base/java.lang.StringBuilder.append(StringBuilder.java:173)\n",
      "\tat org.apache.spark.util.ListenerBus.$anonfun$postToAll$3(ListenerBus.scala:133)\n",
      "\tat org.apache.spark.util.ListenerBus$$Lambda$5066/0x000000080226f7b8.apply(Unknown Source)\n",
      "\tat org.apache.spark.internal.Logging.logInfo(Logging.scala:61)\n",
      "\tat org.apache.spark.internal.Logging.logInfo$(Logging.scala:60)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.logInfo(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:133)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$Lambda$749/0x0000000801543a98.apply$mcJ$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "23/03/26 00:13:25 ERROR Utils: throw uncaught fatal error in thread spark-listener-group-appStatus\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3537)\n",
      "\tat java.base/java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:241)\n",
      "\tat java.base/java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:586)\n",
      "\tat java.base/java.lang.StringBuilder.append(StringBuilder.java:179)\n",
      "\tat scala.collection.mutable.StringBuilder.append(StringBuilder.scala:213)\n",
      "\tat scala.collection.TraversableOnce$appender$1.apply(TraversableOnce.scala:418)\n",
      "\tat scala.collection.TraversableOnce$appender$1.apply(TraversableOnce.scala:410)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.addString(TraversableOnce.scala:424)\n",
      "\tat scala.collection.TraversableOnce.addString$(TraversableOnce.scala:407)\n",
      "\tat scala.collection.AbstractIterator.addString(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.mkString(TraversableOnce.scala:377)\n",
      "\tat scala.collection.TraversableOnce.mkString$(TraversableOnce.scala:376)\n",
      "\tat scala.collection.AbstractIterator.mkString(Iterator.scala:1431)\n",
      "\tat scala.runtime.ScalaRunTime$._toString(ScalaRunTime.scala:165)\n",
      "\tat org.apache.spark.sql.execution.ui.SparkListenerSQLAdaptiveExecutionUpdate.toString(SQLListener.scala:31)\n",
      "\tat java.base/java.lang.String.valueOf(String.java:4225)\n",
      "\tat java.base/java.lang.StringBuilder.append(StringBuilder.java:173)\n",
      "\tat org.apache.spark.util.ListenerBus.$anonfun$postToAll$3(ListenerBus.scala:133)\n",
      "\tat org.apache.spark.util.ListenerBus$$Lambda$5066/0x000000080226f7b8.apply(Unknown Source)\n",
      "\tat org.apache.spark.internal.Logging.logInfo(Logging.scala:61)\n",
      "\tat org.apache.spark.internal.Logging.logInfo$(Logging.scala:60)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.logInfo(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:133)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$Lambda$749/0x0000000801543a98.apply$mcJ$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n"
     ]
    },
    {
     "ename": "java.lang.IllegalStateException",
     "evalue": " Cannot call methods on a stopped SparkContext.",
     "output_type": "error",
     "traceback": [
      "java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.",
      "This stopped SparkContext was created at:",
      "",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)",
      "java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:67)",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:484)",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)",
      "py4j.Gateway.invoke(Gateway.java:238)",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)",
      "java.base/java.lang.Thread.run(Thread.java:1589)",
      "",
      "The currently active SparkContext was created at:",
      "",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)",
      "java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:67)",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:484)",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)",
      "py4j.Gateway.invoke(Gateway.java:238)",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)",
      "java.base/java.lang.Thread.run(Thread.java:1589)",
      "",
      "  at org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:120)",
      "  at org.apache.spark.SparkContext.submitMapStage(SparkContext.scala:2401)",
      "  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:143)",
      "  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:139)",
      "  at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$submitShuffleJob$1(ShuffleExchangeExec.scala:68)",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)",
      "  at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob(ShuffleExchangeExec.scala:68)",
      "  at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob$(ShuffleExchangeExec.scala:67)",
      "  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.submitShuffleJob(ShuffleExchangeExec.scala:115)",
      "  at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture$lzycompute(QueryStageExec.scala:174)",
      "  at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture(QueryStageExec.scala:174)",
      "  at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:176)",
      "  at org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:82)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5(AdaptiveSparkPlanExec.scala:263)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5$adapted(AdaptiveSparkPlanExec.scala:261)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:261)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:233)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:375)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:348)",
      "  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)",
      "  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)",
      "  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2863)",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2870)",
      "  at org.apache.spark.sql.Dataset.first(Dataset.scala:2877)",
      "  at calculate_rank(<console>:49)",
      "  ... 38 elided",
      ""
     ]
    }
   ],
   "source": [
    "var number_of_iterations = 10\n",
    "\n",
    "while(number_of_iterations > 0) {\n",
    "    val contributions = get_contributions(enhanced_rank)\n",
    "    val summed_contributions = explode_and_sum_contributions(contributions)\n",
    "    val new_ranks = calculate_rank(contributions, summed_contributions)\n",
    "    enhanced_rank = new_ranks\n",
    "    number_of_iterations -= 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befacd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_rank.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6237de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
